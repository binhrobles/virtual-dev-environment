---

- hosts: localhost
  connection: local
  gather_facts: false

  vars:
    region: us-east-1
    instance_type: t4g.medium
    image: ami-07ce5684ee3b5482c # ripped from the AWS console
    architecture: arm
    security_group: vdev
    key_name: vdev
    instance_name: vdev-instance
    volume_name: vdev-volume
    instance_profile_name: vdev-instance-profile-manual # manually created, because ansible gets buggy w/ instance profiles

  tasks:
    - name: Provision vdev environment
      # Require the 'provision' tag to provision the env
      tags: ['never', 'provision']
      block:

        - name: Create security group
          tags: ['never', 'provision', 'update-sg']
          amazon.aws.ec2_security_group:
            name: "{{ security_group }}"
            description: SG for virtual dev environment
            region: "{{ region }}"
            rules:
              - proto: tcp
                ports:
                  - 22
                  - 3000
                  - 3001
                  - 3333
                  - 5173  # remix port
                  - 8000
                  - 24678 # vite hmr port
                cidr_ip: 0.0.0.0/0
                rule_desc: allow all on ssh port, dev ports

        - name: Create key pair
          amazon.aws.ec2_key:
            name: "{{ key_name }}"
            region: "{{ region }}"
            key_material: "{{ item }}"
          with_file: /home/binhrobles/.ssh/vdev.pub

        - name: Start an instance
          amazon.aws.ec2_instance:
            name: "{{ instance_name }}"
            key_name: "{{ key_name }}"
            instance_type: "{{ instance_type }}"
            security_group: "{{ security_group }}"
            region: "{{ region }}"
            image_id: "{{ image }}"
            iam_instance_profile: "{{ instance_profile_name }}"
            wait: true
          register: ec2

        - name: Associate static Elastic IP
          amazon.aws.ec2_eip:
            device_id: "{{ ec2.instances[0].instance_id }}"

          # some idempotency issues here
        - name: Create / Attach EBS volume
          ignore_errors: true
          amazon.aws.ec2_vol:
            instance: "{{ ec2.instances[0].instance_id }}"
            name: "{{ volume_name }}"
            device_name: /dev/sdf
            volume_type: gp3
            volume_size: 30
            delete_on_termination: true

        - name: Get vdev facts
          tags: ['alarm']
          amazon.aws.ec2_instance_info:
            filters:
              "tag:Name": vdev-instance
          register: ec2_info

          # set up auto shutdown
        - name: Auto stop if CPU usage is less than 3% for 45 minutes
          tags: ['alarm']
          ec2_metric_alarm:
            state: present
            region: us-east-1
            name: "cpu-low"
            metric: "CPUUtilization"
            namespace: "AWS/EC2"
            statistic: Average
            comparison: "LessThanOrEqualToThreshold"
            threshold: 3.0
            period: 300
            evaluation_periods: 9
            unit: "Percent"
            description: "This will alarm when vdev's cpu usage average is lower than 3% for 45 minutes "
            dimensions: { 'InstanceId': "{{ ec2_info.instances[0].instance_id }}" }
            alarm_actions: [ "arn:aws:automate:us-east-1:ec2:stop" ]

        - name: Instance info
          debug:
            msg: "ID: {{ ec2_info.instances[0].instance_id }} - State: {{ ec2_info.instances[0].state.name }} - Public DNS: {{ ec2_info.instances[0].public_dns_name }}"

    - name: Stop vdev instance
      tags: ['never', 'stop']
      amazon.aws.ec2_instance:
        name: vdev-instance
        state: stopped
    
    - name: Start vdev instance
      tags: ['never', 'start']
      amazon.aws.ec2_instance:
        name: vdev-instance
        state: running

    - name: Nuke vdev environment completely
      # Require the 'nuke' tag to nuke the env
      tags: ['never', 'nuke']
      block:

        - name: Get vdev facts
          amazon.aws.ec2_instance_info:
            filters:
              "tag:Name": vdev-instance
          register: ec2

        - name: Release Elastic IP
          amazon.aws.ec2_eip:
            device_id: "{{ ec2.instances[0].instance_id }}"
            state: absent

        - name: Kill vdev instance
          amazon.aws.ec2_instance:
            name: vdev-instance
            state: absent

        - name: Destroy security group
          amazon.aws.ec2_security_group:
            name: "{{ security_group }}"
            state: absent

        - name: Destroy key pair
          amazon.aws.ec2_key:
            name: "{{ key_name }}"
            region: "{{ region }}"
            state: absent

- hosts: tag_Name_vdev_instance
  gather_facts: false

  tasks:
    - name: Set up dev environment
      tags: ['never', 'configure']
      block:

        - name: Install yum dependencies
          become: true
          yum:
            name:
              - git
              - gcc-c++
              - make
              - tmux
              - amazon-cloudwatch-agent
              - yum-utils
              - shadow-utils
              - docker
              - zsh
            state: present

        - name: install autojump
          shell:
            cmd: |
              git clone https://github.com/wting/autojump.git
              pushd autojump; ./install.py
            creates: /home/ec2-user/.autojump/bin/autojump

        - name: install ngrok
          become: true
          shell:
            cmd: |
              wget https://bin.equinox.io/c/bNyj1mQVY4c/ngrok-v3-stable-linux-arm64.tgz
              sudo tar xvzf ./ngrok-v3-stable-linux-arm64.tgz -C /usr/local/bin
            creates: /usr/local/bin/ngrok

        - name: install docker-compose
          become: true
          shell:
            cmd: |
              curl -L https://github.com/docker/compose/releases/latest/download/docker-compose-$(uname -s)-$(uname -m) -o /usr/local/bin/docker-compose
              chmod +x /usr/local/bin/docker-compose
            creates: /usr/local/bin/docker-compose

        - name: install oh my zsh
          shell:
            cmd: |
              sh -c "$(curl -fsSL https://raw.githubusercontent.com/ohmyzsh/ohmyzsh/master/tools/install.sh)"
            creates: /home/ec2-user/.oh-my-zsh

        - name: change default shell
          become: true
          shell: which zsh | lchsh ec2-user

        - name: Copy some dotfiles
          tags: ['update-dotfiles']
          copy:
            src: "{{ item.src }}"
            dest: "{{ item.dest }}"
            owner: ec2-user
            group: ec2-user
            mode: preserve
          loop:
            - src: ~/.tmux.conf
              dest: /home/ec2-user/.tmux.conf
            - src: ~/.zshrc
              dest: /home/ec2-user/.zshrc
            - src: ~/.ssh/github
              dest: /home/ec2-user/.ssh/github
            - src: ~/.gitconfig
              dest: /home/ec2-user/.gitconfig

        # add hashicorp to yum repos
        # for some reason, yum_repository adds the repo, 
        # but yum install terraform won't work
        - name: Add hashicorp repository
          become: true
          command: 
            cmd: yum-config-manager --add-repo https://rpm.releases.hashicorp.com/AmazonLinux/hashicorp.repo
            creates: /etc/yum.repos.d/hashicorp.repo

        # install terraform
        - name: Install Terraform
          become: true
          yum:
            name: terraform
            state: present

        - name: Copy CloudWatch config
          become: true
          copy:
            src: amazon-cloudwatch-agent.json
            dest: /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json
            owner: cwagent
            group: cwagent
            mode: 0644
          register: cw_config

        - name: Start CloudWatch agent
          become: true
          command: /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json -s
          when: cw_config.changed

          # might get flaky
        - name: mount file system
          become: true
          shell:
            cmd: |
              mkfs -t xfs /dev/sdf
              mkdir /workspace
              echo "$(blkid -o export /dev/sdf | grep ^UUID=) /workspace xfs defaults,noatime" >> /etc/fstab
              mount -a
              chmod 700 /workspace
              ln -s /workspace .
              chown ec2-user:ec2-user /workspace
            creates: /workspace

        - name: Install Node.js via nvm + enable Yarn via corepack
          shell:
            cmd: |
              curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash
              source ~/.bashrc
              nvm install --lts
              nvm use --lts
              corepack enable
            creates: /home/ec2-user/.nvm/alias
